# -*- coding: utf-8 -*-
"""TP522.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZzZGTXmRCHFN67728ZxOgSZWeJy4eP3
"""

from numpy.lib.polynomial import polyval
from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings("ignore")
import seaborn as sns
from sklearn.preprocessing import StandardScaler
scale=StandardScaler()

df = pd.read_csv("diabetes(1).csv")

val=0
lis=[]
for i in range(8):
  for j in range(8):
    X = df.iloc[:, [i,j]].values
    #os=scale.fit(X).transform(X)
    #print(i,j)
    y = df.iloc[:, -1].values
    # On conserve 50% du jeu de données pour l'évaluation
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,random_state=0)
    C = 100.0 # paramètre de régularisation
    lin_svc = svm.SVC(C=C,kernel="rbf")
    y_pred=lin_svc.fit(X_train, y_train)
    p=y_pred.score(X_test,y_test)
    if val < p :
      val=p
      lis.append((i,j))
    #print(p,val)
tes=0
for i in range(1,2000,50):
  X = df.iloc[:, [lis[-1][1],lis[-1][0]]].values
  os=scale.fit(X).transform(X)
  X_train, X_test, y_train, y_test = train_test_split(os, y, test_size=0.5,random_state=0)
  C = i # paramètre de régularisation
  lin_svc = svm.SVC(C=C,kernel="rbf")
  y_pred=lin_svc.fit(X_train, y_train)
  p=y_pred.score(X_train,y_train)
  if (tes<p):
    tes=p
    C1=C
print(val,lis,p,C1)
sns.pairplot(df.iloc[:, [lis[-1][0],lis[-1][1]]])

from sklearn.model_selection import GridSearchCV

X = df.iloc[:, [lis[-1][1],lis[-1][0]]].values
gamma=0.5
C=1
os=scale.fit(X).transform(X)
X_train, X_test, y_train, y_test = train_test_split(os, y, test_size=0.5,random_state=0) # paramètre de régularisation
lin_svc = svm.SVC(kernel='rbf',C=C, gamma=gamma)
y_pred=lin_svc.fit(X_train, y_train)
p=y_pred.score(X_train,y_train)
plt.subplot()
plt.figure()
print(p,C)
# Créer la surface de décision discretisée
x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1

# Pour afficher la surface de décision on va discrétiser l'espace avec un
h = max((x_max - x_min) / 100, (y_max - y_min) / 100)
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Surface de décision
Z = lin_svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# Afficher aussi les points d'apprentissage

plt.scatter(X_train[:, 0], X_train[:, 1], label="train", edgecolors='k',c=y_train, cmap=plt.cm.coolwarm)
plt.scatter(X_test[:, 0], X_test[:, 1], label="test", marker="*",c=y_test,cmap=plt.cm.coolwarm)

plt.title("LinearSVC")
tuned = {"gamma":[0.1,0.2,0.4,0.5,0.6,0.7,0.8,0.9,1]}

treee=svm.SVC(kernel="rbf")
clf = GridSearchCV(treee, tuned) 
clf.fit(X_train, y_train)

print(clf.best_params_)

